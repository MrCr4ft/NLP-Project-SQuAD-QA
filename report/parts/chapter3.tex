\chapter{System Description} 
The architecture of QANet can be decomposed in 5 main building blocks: an input embedding layer, an encoding embedding layer, a context-query attention layer, a model encoder layer and a output layer.
1. Input embedding layer
In this layer we extract the word and character embeddings using their indices; the word embeddings are 300-dimensional vectors, we used the pre-trained Glove embeddings, and the character embeddings
are 200-dimensional vectors that are initialized randomly and are trainable. The OOV words are mapped to a vector of zeros, in the paper this embedding was made trainable while in our implementation it's fixed.
The character embeddings of every word is passed through convolutional layers, in order to aggregate the information between characters and obtain a new representation of the word. At the end of this layer we'll 
have the concatenation of word embeddings and character embeddings. This new representation is passed through an Highway network {CIT HIGHWAY}, which has the purpose of facilitating the flow of information,
through the usage of gating units, that helps with the training of deep neural networks.
2. Embedding encoder layer
This layer is composed by stacking different encoders, that can be decomposed in: convolutional layer, self-attention layer and feed forward layer. Each of this layers is preceded by a normalization layer,
as proposed in {CIT LAYERNORM}, and wrapped inside a residual block. The dimension of the word embedding in input in this layer is mapped into a lower dimension by one-dimensional convolution.
We implemented the convolutions in the encoder block as depth-wise separable, as in the paper. The multi-head attention mechanism that we've used is the one defined in {CIT TRANSFORMER}, which transform each word
embedding in three new representations, query, key and values, then computes the similarity between a query and all the possible keys and use these similarity values to compute a new representation of the word.
3. Context-Query attention
Here the information between context and query is aggregated using attention. This is done by computing the similarity matrix [S ..], filled with the similarities between each pair of context and query words, that
is then normalized with a softmax. THe context-to-query attention is computed as [A ..]. The similarity funcion used is the trilinear function. Furthermore, we used also a query-to-context attention; in order 
to do this we computed the column normalized matrix [S--] of S with a softmax, then the query-to-context is computed as [B--].
The output of this layer is, for each word, given by [c, a, c*a, c*b] where a and b are rows of A and B. 
4. Model encoder layer
In this layer we use the same encoder structure used in the Embedding encoder layer. The output of this layer is given by a concatenation of the outputs of the different blocks, in particular we concatenated
the output of the first and second block, and the output of the first and third block. Because of the concatenation, the dimension of the embedding in input is larger, so we decided to bring it down with a one
dimensional convolution.
5. Output layer
The two vector obtained in the previous layer are passed into two separate linear layers with a softmax at the end. This allow to compute the start and the end of an answer span. The loss function used
is a sum of the cross-entropy loss function computed both on the star position and the end position probabilities.
At inference time the span is chosen such that the product of the probabilities of the indices is maximized.

In order to deal with the padding of the sequences, we defined a mask that is passed whenever it's necessary to compute a softmax function, i.e. in the attention(s) computation and in the output layer.